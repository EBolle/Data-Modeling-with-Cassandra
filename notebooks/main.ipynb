{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Welcome to the Cassandra project\n",
    "\n",
    "The goal of the project is to transform a number of .csv files to 3 optimized Cassandra tables, ready to execute 3 specific queries. \n",
    "To reach this goal we need to do the following:\n",
    "\n",
    "1. transform the .csv files to a Pandas DataFrame\n",
    "2. inspect the distribution of the potential partition keys\n",
    "3. check for uniqueness of the primary keys\n",
    "4. create the database\n",
    "5. transform the Pandas DataFrame to lists of tuples, ready for insertion into the database\n",
    "6. insert the data\n",
    "7. inspect and validate the queries\n",
    "8. close the session and cluster connection \n",
    "\n",
    "**NOTE**: run all queries, check all queries, validate the idea behind them (order by) and the table names. If satisfied:\n",
    "- clean this notebook and transfer to PyCharm / git\n",
    "- write a decent but short readme -> most information comes from the notebook -> copy / paste from Postgres when possible\n",
    "- check the graph functions in the project\n",
    "- Check the Project_1B notebook once more\n",
    "- Submit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "! pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 1. transform the .csv files to a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from src.create_event_data import CreateEventData\n",
    "from src.data_utils import create_csv_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../event_data contains 31 .csv files.\n"
     ]
    }
   ],
   "source": [
    "event_data_path = Path('..') / 'event_data'\n",
    "csv_list = sorted(create_csv_path_list(event_data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "del csv_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2021-01-04 12:57:15,524 [create_event_data.py:data_pipeline:37] Data pipeline started...\n",
      "DEBUG 2021-01-04 12:57:15,554 [create_event_data.py:data_pipeline:46] Processed 1 / 30 .csv files\n",
      "DEBUG 2021-01-04 12:57:15,569 [create_event_data.py:data_pipeline:46] Processed 2 / 30 .csv files\n",
      "DEBUG 2021-01-04 12:57:15,584 [create_event_data.py:data_pipeline:46] Processed 3 / 30 .csv files\n",
      "DEBUG 2021-01-04 12:57:15,599 [create_event_data.py:data_pipeline:46] Processed 4 / 30 .csv files\n",
      "DEBUG 2021-01-04 12:57:15,616 [create_event_data.py:data_pipeline:46] Processed 5 / 30 .csv files\n",
      "DEBUG 2021-01-04 12:57:15,631 [create_event_data.py:data_pipeline:46] Processed 6 / 30 .csv files\n",
      "DEBUG 2021-01-04 12:57:15,647 [create_event_data.py:data_pipeline:46] Processed 7 / 30 .csv files\n",
      "DEBUG 2021-01-04 12:57:15,664 [create_event_data.py:data_pipeline:46] Processed 8 / 30 .csv files\n",
      "DEBUG 2021-01-04 12:57:15,684 [create_event_data.py:data_pipeline:46] Processed 9 / 30 .csv files\n",
      "DEBUG 2021-01-04 12:57:15,699 [create_event_data.py:data_pipeline:46] Processed 10 / 30 .csv files\n",
      "DEBUG 2021-01-04 12:57:15,715 [create_event_data.py:data_pipeline:46] Processed 11 / 30 .csv files\n",
      "DEBUG 2021-01-04 12:57:15,731 [create_event_data.py:data_pipeline:46] Processed 12 / 30 .csv files\n",
      "DEBUG 2021-01-04 12:57:15,749 [create_event_data.py:data_pipeline:46] Processed 13 / 30 .csv files\n",
      "DEBUG 2021-01-04 12:57:15,767 [create_event_data.py:data_pipeline:46] Processed 14 / 30 .csv files\n",
      "DEBUG 2021-01-04 12:57:15,788 [create_event_data.py:data_pipeline:46] Processed 15 / 30 .csv files\n",
      "DEBUG 2021-01-04 12:57:15,806 [create_event_data.py:data_pipeline:46] Processed 16 / 30 .csv files\n",
      "DEBUG 2021-01-04 12:57:15,824 [create_event_data.py:data_pipeline:46] Processed 17 / 30 .csv files\n",
      "DEBUG 2021-01-04 12:57:15,840 [create_event_data.py:data_pipeline:46] Processed 18 / 30 .csv files\n",
      "DEBUG 2021-01-04 12:57:15,858 [create_event_data.py:data_pipeline:46] Processed 19 / 30 .csv files\n",
      "DEBUG 2021-01-04 12:57:15,876 [create_event_data.py:data_pipeline:46] Processed 20 / 30 .csv files\n",
      "DEBUG 2021-01-04 12:57:15,894 [create_event_data.py:data_pipeline:46] Processed 21 / 30 .csv files\n",
      "DEBUG 2021-01-04 12:57:15,911 [create_event_data.py:data_pipeline:46] Processed 22 / 30 .csv files\n",
      "DEBUG 2021-01-04 12:57:15,929 [create_event_data.py:data_pipeline:46] Processed 23 / 30 .csv files\n",
      "DEBUG 2021-01-04 12:57:15,947 [create_event_data.py:data_pipeline:46] Processed 24 / 30 .csv files\n",
      "DEBUG 2021-01-04 12:57:15,963 [create_event_data.py:data_pipeline:46] Processed 25 / 30 .csv files\n",
      "DEBUG 2021-01-04 12:57:15,982 [create_event_data.py:data_pipeline:46] Processed 26 / 30 .csv files\n",
      "DEBUG 2021-01-04 12:57:16,001 [create_event_data.py:data_pipeline:46] Processed 27 / 30 .csv files\n",
      "DEBUG 2021-01-04 12:57:16,021 [create_event_data.py:data_pipeline:46] Processed 28 / 30 .csv files\n",
      "DEBUG 2021-01-04 12:57:16,039 [create_event_data.py:data_pipeline:46] Processed 29 / 30 .csv files\n",
      "DEBUG 2021-01-04 12:57:16,059 [create_event_data.py:data_pipeline:46] Processed 30 / 30 .csv files\n"
     ]
    }
   ],
   "source": [
    "create_event_instance = CreateEventData(csv_path_list=csv_list)\n",
    "event_df = create_event_instance.data_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>firstName</th>\n",
       "      <th>gender</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>lastName</th>\n",
       "      <th>length</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>song</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Des'ree</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>Summers</td>\n",
       "      <td>246.30812</td>\n",
       "      <td>free</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "      <td>139</td>\n",
       "      <td>You Gotta Be</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mr Oizo</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>F</td>\n",
       "      <td>3</td>\n",
       "      <td>Summers</td>\n",
       "      <td>144.03873</td>\n",
       "      <td>free</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "      <td>139</td>\n",
       "      <td>Flat 55</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tamba Trio</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>F</td>\n",
       "      <td>4</td>\n",
       "      <td>Summers</td>\n",
       "      <td>177.18812</td>\n",
       "      <td>free</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "      <td>139</td>\n",
       "      <td>Quem Quiser Encontrar O Amor</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       artist firstName gender  itemInSession lastName     length level  \\\n",
       "0     Des'ree    Kaylee      F              1  Summers  246.30812  free   \n",
       "1     Mr Oizo    Kaylee      F              3  Summers  144.03873  free   \n",
       "2  Tamba Trio    Kaylee      F              4  Summers  177.18812  free   \n",
       "\n",
       "                      location  sessionId                          song  \\\n",
       "0  Phoenix-Mesa-Scottsdale, AZ        139                  You Gotta Be   \n",
       "1  Phoenix-Mesa-Scottsdale, AZ        139                       Flat 55   \n",
       "2  Phoenix-Mesa-Scottsdale, AZ        139  Quem Quiser Encontrar O Amor   \n",
       "\n",
       "   userId  \n",
       "0       8  \n",
       "1       8  \n",
       "2       8  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare to the .jpg in /images\n",
    "event_df.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6820 entries, 0 to 6819\n",
      "Data columns (total 11 columns):\n",
      "artist           6820 non-null object\n",
      "firstName        6820 non-null object\n",
      "gender           6820 non-null object\n",
      "itemInSession    6820 non-null int64\n",
      "lastName         6820 non-null object\n",
      "length           6820 non-null float64\n",
      "level            6820 non-null object\n",
      "location         6820 non-null object\n",
      "sessionId        6820 non-null int64\n",
      "song             6820 non-null object\n",
      "userId           6820 non-null int64\n",
      "dtypes: float64(1), int64(3), object(7)\n",
      "memory usage: 586.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# check correct datatypes\n",
    "event_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 2. inspect the distribution of the potential partition keys\n",
    "\n",
    "The potential partition keys are dependent on the queries we are interested in, these keys are marked in **bold**. These partition keys will be part of the WHERE statements in the SQL queries. To fully benefit from the speed of a Cassandra database, it is important that the partition key is evenly distributed, so that each node has a comparable number of values to store. If only one node contains most of the values, this may slow down the queries.\n",
    "To prevent this, we visually inspect each potential partition key with the help of matplotlib and seaborn (the code to reproduce these graphs can be found in src/partition_key_graphs.py).\n",
    "\n",
    "1. Give me the artist, song title and song's length in the music app history that was heard during **sessionId** = 338, and **itemInSession**  = 4\n",
    "2. Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for **userid** = 10, **sessionid** = 182\n",
    "3. Give me every user name (first and last) in my music app history who listened to the **song** 'All Hands Against His Own'\n",
    "\n",
    "#### Query 1\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/49920622/103486947-d7f4a380-4e01-11eb-8b81-6c7494649b28.jpg\" width=800>\n",
    "\n",
    "#### Query 2\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/49920622/103487010-581b0900-4e02-11eb-821b-32951b566c4b.jpg\" width=800>\n",
    "\n",
    "#### Query 3\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/49920622/103487072-d7104180-4e02-11eb-9689-529824ba78d2.jpg\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 3. check for uniqueness of the primary keys\n",
    "\n",
    "Based on the graphs we have found our partition key, but our primary key consists of clustering keys as well. The combination of the partition and clustering key makes up for the primary index, and this combination *must* be unique. Before we create the tables, we need to verify this constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from src.data_utils import unique_key_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_key_check(event_df, ['sessionId', 'itemInSession'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_key_check(event_df, ['userId', 'sessionId', 'itemInSession'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_key_check(event_df, ['song'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Note\n",
    "\n",
    "Queries 1 and 2 are unique based on the already identified keys, however, for query 3 the variable song is not unique by itself. Since people love to hear certain songs more than once - even in the same session -, we add sessionId *and* itemInSession as clustering keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_key_check(event_df, ['song', 'sessionId', 'itemInSession'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4. create the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from src.create_database import CreateDatabase\n",
    "from src.sql_queries import drop_list, create_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "create_db_instance = CreateDatabase(create_queries=create_list, drop_queries=drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2021-01-04 12:57:29,648 [create_database.py:create_database_pipeline:20] Create database pipeline started...\n",
      "INFO 2021-01-04 12:57:29,650 [create_database.py:init_cluster_and_session:30] Initializing the local Cassandra cluster and session\n",
      "INFO 2021-01-04 12:57:29,769 [create_database.py:set_udacity_keyspace:43] Setting up the udacity keyspace\n",
      "INFO 2021-01-04 12:57:29,777 [create_database.py:drop_tables:59] Dropping tables if exists\n",
      "INFO 2021-01-04 12:57:30,524 [create_database.py:create_tables:68] Creating tables\n"
     ]
    }
   ],
   "source": [
    "cluster, session = create_db_instance.create_database_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 5. transform the Pandas DataFrame to lists of tuples, ready for insertion into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# query 1 -> session_info table \n",
    "columns_session_info = ['artist', 'song', 'length', 'sessionId', 'itemInSession']\n",
    "data_session_info = [tuple(row) for row in event_df[columns_session_info].itertuples(index=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# query 2 -> user_info table \n",
    "columns_user_info = ['artist', 'song', 'firstName', 'lastName', 'sessionId', 'userId', 'itemInSession']\n",
    "data_user_info = [tuple(row) for row in event_df[columns_user_info].itertuples(index=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# query 3 -> user_info_per_song table \n",
    "columns_user_info_per_song = ['firstName', 'lastName', 'song', 'sessionId', 'itemInSession']\n",
    "data_user_info_per_song = [tuple(row) for row in event_df[columns_user_info_per_song].itertuples(index=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 6. insert the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from src.sql_queries import insert_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserting file at index 0/3\n",
      "inserting file at index 1/3\n",
      "inserting file at index 2/3\n"
     ]
    }
   ],
   "source": [
    "data_list = [data_session_info, data_user_info, data_user_info_per_song]\n",
    "\n",
    "for idx, (data, query) in enumerate(zip(data_list, insert_list), start=1):\n",
    "    print(f\"inserting file {idx}/{len(data_list)}\")\n",
    "    for row in data:\n",
    "        try:\n",
    "            session.execute(query, row)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 7. inspect and validate the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Create the Cassandra tables\n",
    "\n",
    "##### Give the artist, song title and song's length in the music app history that was heard during  sessionId = 338, and itemInSession  = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "create_query_1 = \"\"\" \n",
    "CREATE TABLE IF NOT EXISTS item_in_session (\n",
    "artist text,\n",
    "song text,\n",
    "song_length float,\n",
    "session_id int,\n",
    "item_in_session int,\n",
    "PRIMARY KEY (session_id, item_in_session)\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    session.execute(create_query_1)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "insert_query_1 = \"\"\"\n",
    "INSERT INTO song_length (artist, song, song_length, session_id, item_in_session)\n",
    "VALUES (%s, %s, %s, %s, %s)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_query_1 = [tuple(row) for row in return_df[['artist', 'song', 'length', 'sessionId', 'itemInSession']].itertuples(index=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_query_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "for row in data_query_1:\n",
    "    try:\n",
    "        session.execute(insert_query_1, row)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    rows = session.execute(\"SELECT * FROM song_length\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### NOTES\n",
    "- rewrite preprocessing, difference create / insert / and DATA -> easier to create the list of tuples per query ... so only return RETURN DF\n",
    "- from there create functions to create the list of tuples per query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Close the session and cluster connection -> refer to the instance variables of create_db_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
