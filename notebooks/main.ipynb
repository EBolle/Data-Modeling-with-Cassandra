{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the Cassandra project\n",
    "\n",
    "The goal of the project is to transform a number of .csv files to 3 optimized Cassandra tables, ready to execute 3 specific queries. \n",
    "To reach this goal we need to do the following:\n",
    "\n",
    "1. transform the .csv files to a Pandas DataFrame\n",
    "2. inspect the distribution of the potential partition keys\n",
    "3. check for uniqueness of the primary keys\n",
    "4. create the database\n",
    "5. transform the Pandas DataFrame to lists of tuples, ready for insertion into the database\n",
    "6. insert the data\n",
    "7. inspect and validate the queries\n",
    "8. close the session and cluster connection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. transform the .csv files to a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from src.create_event_data import CreateEventData\n",
    "from src.data_utils import create_csv_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\event_data contains 30 .csv files.\n"
     ]
    }
   ],
   "source": [
    "event_data_path = Path('..') / 'event_data'\n",
    "csv_list = sorted(create_csv_path_list(event_data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2021-01-05 10:39:47,689 [create_event_data.py:data_pipeline:37] Data pipeline started...\n",
      "DEBUG 2021-01-05 10:39:47,736 [create_event_data.py:data_pipeline:46] Processed 1 / 30 .csv files\n",
      "DEBUG 2021-01-05 10:39:47,758 [create_event_data.py:data_pipeline:46] Processed 2 / 30 .csv files\n",
      "DEBUG 2021-01-05 10:39:47,780 [create_event_data.py:data_pipeline:46] Processed 3 / 30 .csv files\n",
      "DEBUG 2021-01-05 10:39:47,803 [create_event_data.py:data_pipeline:46] Processed 4 / 30 .csv files\n",
      "DEBUG 2021-01-05 10:39:47,829 [create_event_data.py:data_pipeline:46] Processed 5 / 30 .csv files\n",
      "DEBUG 2021-01-05 10:39:47,852 [create_event_data.py:data_pipeline:46] Processed 6 / 30 .csv files\n",
      "DEBUG 2021-01-05 10:39:47,874 [create_event_data.py:data_pipeline:46] Processed 7 / 30 .csv files\n",
      "DEBUG 2021-01-05 10:39:47,907 [create_event_data.py:data_pipeline:46] Processed 8 / 30 .csv files\n",
      "DEBUG 2021-01-05 10:39:47,936 [create_event_data.py:data_pipeline:46] Processed 9 / 30 .csv files\n",
      "DEBUG 2021-01-05 10:39:47,958 [create_event_data.py:data_pipeline:46] Processed 10 / 30 .csv files\n",
      "DEBUG 2021-01-05 10:39:47,983 [create_event_data.py:data_pipeline:46] Processed 11 / 30 .csv files\n",
      "DEBUG 2021-01-05 10:39:48,005 [create_event_data.py:data_pipeline:46] Processed 12 / 30 .csv files\n",
      "DEBUG 2021-01-05 10:39:48,031 [create_event_data.py:data_pipeline:46] Processed 13 / 30 .csv files\n",
      "DEBUG 2021-01-05 10:39:48,058 [create_event_data.py:data_pipeline:46] Processed 14 / 30 .csv files\n",
      "DEBUG 2021-01-05 10:39:48,086 [create_event_data.py:data_pipeline:46] Processed 15 / 30 .csv files\n",
      "DEBUG 2021-01-05 10:39:48,115 [create_event_data.py:data_pipeline:46] Processed 16 / 30 .csv files\n",
      "DEBUG 2021-01-05 10:39:48,137 [create_event_data.py:data_pipeline:46] Processed 17 / 30 .csv files\n",
      "DEBUG 2021-01-05 10:39:48,163 [create_event_data.py:data_pipeline:46] Processed 18 / 30 .csv files\n",
      "DEBUG 2021-01-05 10:39:48,189 [create_event_data.py:data_pipeline:46] Processed 19 / 30 .csv files\n",
      "DEBUG 2021-01-05 10:39:48,216 [create_event_data.py:data_pipeline:46] Processed 20 / 30 .csv files\n",
      "DEBUG 2021-01-05 10:39:48,244 [create_event_data.py:data_pipeline:46] Processed 21 / 30 .csv files\n",
      "DEBUG 2021-01-05 10:39:48,267 [create_event_data.py:data_pipeline:46] Processed 22 / 30 .csv files\n",
      "DEBUG 2021-01-05 10:39:48,292 [create_event_data.py:data_pipeline:46] Processed 23 / 30 .csv files\n",
      "DEBUG 2021-01-05 10:39:48,320 [create_event_data.py:data_pipeline:46] Processed 24 / 30 .csv files\n",
      "DEBUG 2021-01-05 10:39:48,344 [create_event_data.py:data_pipeline:46] Processed 25 / 30 .csv files\n",
      "DEBUG 2021-01-05 10:39:48,372 [create_event_data.py:data_pipeline:46] Processed 26 / 30 .csv files\n",
      "DEBUG 2021-01-05 10:39:48,399 [create_event_data.py:data_pipeline:46] Processed 27 / 30 .csv files\n",
      "DEBUG 2021-01-05 10:39:48,430 [create_event_data.py:data_pipeline:46] Processed 28 / 30 .csv files\n",
      "DEBUG 2021-01-05 10:39:48,469 [create_event_data.py:data_pipeline:46] Processed 29 / 30 .csv files\n",
      "DEBUG 2021-01-05 10:39:48,498 [create_event_data.py:data_pipeline:46] Processed 30 / 30 .csv files\n"
     ]
    }
   ],
   "source": [
    "create_event_instance = CreateEventData(csv_path_list=csv_list)\n",
    "event_df = create_event_instance.data_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>firstName</th>\n",
       "      <th>gender</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>lastName</th>\n",
       "      <th>length</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>song</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Des'ree</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>Summers</td>\n",
       "      <td>246.30812</td>\n",
       "      <td>free</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "      <td>139</td>\n",
       "      <td>You Gotta Be</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mr Oizo</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>F</td>\n",
       "      <td>3</td>\n",
       "      <td>Summers</td>\n",
       "      <td>144.03873</td>\n",
       "      <td>free</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "      <td>139</td>\n",
       "      <td>Flat 55</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tamba Trio</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>F</td>\n",
       "      <td>4</td>\n",
       "      <td>Summers</td>\n",
       "      <td>177.18812</td>\n",
       "      <td>free</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "      <td>139</td>\n",
       "      <td>Quem Quiser Encontrar O Amor</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       artist firstName gender  itemInSession lastName     length level  \\\n",
       "0     Des'ree    Kaylee      F              1  Summers  246.30812  free   \n",
       "1     Mr Oizo    Kaylee      F              3  Summers  144.03873  free   \n",
       "2  Tamba Trio    Kaylee      F              4  Summers  177.18812  free   \n",
       "\n",
       "                      location  sessionId                          song  \\\n",
       "0  Phoenix-Mesa-Scottsdale, AZ        139                  You Gotta Be   \n",
       "1  Phoenix-Mesa-Scottsdale, AZ        139                       Flat 55   \n",
       "2  Phoenix-Mesa-Scottsdale, AZ        139  Quem Quiser Encontrar O Amor   \n",
       "\n",
       "   userId  \n",
       "0       8  \n",
       "1       8  \n",
       "2       8  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare to the .jpg in /images\n",
    "event_df.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6820 entries, 0 to 6819\n",
      "Data columns (total 11 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   artist         6820 non-null   object \n",
      " 1   firstName      6820 non-null   object \n",
      " 2   gender         6820 non-null   object \n",
      " 3   itemInSession  6820 non-null   int32  \n",
      " 4   lastName       6820 non-null   object \n",
      " 5   length         6820 non-null   float64\n",
      " 6   level          6820 non-null   object \n",
      " 7   location       6820 non-null   object \n",
      " 8   sessionId      6820 non-null   int32  \n",
      " 9   song           6820 non-null   object \n",
      " 10  userId         6820 non-null   int32  \n",
      "dtypes: float64(1), int32(3), object(7)\n",
      "memory usage: 506.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# check correct datatypes\n",
    "event_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. inspect the distribution of the potential partition keys\n",
    "\n",
    "The potential partition keys are dependent on the queries we are interested in, these keys are marked in **bold**. These partition keys will be part of the WHERE statements in the SQL queries. To fully benefit from the speed of a Cassandra database, it is important that the partition key is evenly distributed, so that each node has a comparable number of values to store. If only one node contains most of the values, this may slow down the queries.\n",
    "To prevent this, we visually inspect each potential partition key with the help of matplotlib and seaborn (the code to reproduce these graphs can be found in src/partition_key_graphs.py).\n",
    "\n",
    "1. Give me the artist, song title and song's length in the music app history that was heard during **sessionId** = 338, and **itemInSession**  = 4\n",
    "2. Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for **userid** = 10, **sessionid** = 182\n",
    "3. Give me every user name (first and last) in my music app history who listened to the **song** 'All Hands Against His Own'\n",
    "\n",
    "#### Query 1\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/49920622/103486947-d7f4a380-4e01-11eb-8b81-6c7494649b28.jpg\" width=800>\n",
    "\n",
    "#### Query 2\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/49920622/103487010-581b0900-4e02-11eb-821b-32951b566c4b.jpg\" width=800>\n",
    "\n",
    "#### Query 3\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/49920622/103487072-d7104180-4e02-11eb-9689-529824ba78d2.jpg\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. check for uniqueness of the primary keys\n",
    "\n",
    "Based on the graphs we have found the best suited partition keys. However, by themselves they cannot uniquely identify each row. Furthermore, we also need to make multiple statements in the WHERE clause. Both of these reasons leads to the addition of clustering keys. The combination of the partition and clustering key make up for the unique primary index. Before we create the tables, we need to verify this assumed uniqueness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_utils import unique_key_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_key_check(event_df, ['sessionId', 'itemInSession'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_key_check(event_df, ['userId', 'sessionId', 'itemInSession'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_key_check(event_df, ['song'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "\n",
    "Queries 1 and 2 are unique based on the already identified keys, however, for query 3 the variable song is not unique by itself. Since people love to hear certain songs more than once - even in the same session -, we add sessionId *and* itemInSession as clustering keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_key_check(event_df, ['song', 'sessionId', 'itemInSession'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. create the database\n",
    "\n",
    "The create_database_pipeline method returns the cluster and session instances so we can use these to execute the insert queries in the next step, and to shutdown the cluster and session at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.create_database import CreateDatabase\n",
    "from src.sql_queries import drop_list, create_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_db_instance = CreateDatabase(create_queries=create_list, drop_queries=drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2021-01-05 09:22:55,271 [create_database.py:create_database_pipeline:20] Create database pipeline started...\n",
      "INFO 2021-01-05 09:22:55,275 [create_database.py:init_cluster_and_session:30] Initializing the local Cassandra cluster and session\n",
      "INFO 2021-01-05 09:22:55,539 [create_database.py:set_udacity_keyspace:43] Setting up the udacity keyspace\n",
      "INFO 2021-01-05 09:22:55,652 [create_database.py:drop_tables:59] Dropping tables if exists\n",
      "INFO 2021-01-05 09:22:55,665 [create_database.py:create_tables:68] Creating tables\n"
     ]
    }
   ],
   "source": [
    "cluster, session = create_db_instance.create_database_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. transform the Pandas DataFrame to lists of tuples, ready for insertion into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query 1 -> session_info table \n",
    "columns_session_info = ['artist', 'song', 'length', 'sessionId', 'itemInSession']\n",
    "data_session_info = [tuple(row) for row in event_df[columns_session_info].itertuples(index=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query 2 -> user_info table \n",
    "columns_user_info = ['artist', 'song', 'firstName', 'lastName', 'sessionId', 'userId', 'itemInSession']\n",
    "data_user_info = [tuple(row) for row in event_df[columns_user_info].itertuples(index=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query 3 -> user_info_per_song table \n",
    "columns_user_info_per_song = ['firstName', 'lastName', 'song', 'sessionId', 'itemInSession']\n",
    "data_user_info_per_song = [tuple(row) for row in event_df[columns_user_info_per_song].itertuples(index=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. insert the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sql_queries import insert_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserting file 1/3\n",
      "inserting file 2/3\n",
      "inserting file 3/3\n"
     ]
    }
   ],
   "source": [
    "data_list = [data_session_info, data_user_info, data_user_info_per_song]\n",
    "\n",
    "for idx, (data, query) in enumerate(zip(data_list, insert_list), start=1):\n",
    "    print(f\"inserting file {idx}/{len(data_list)}\")\n",
    "    for row in data:\n",
    "        try:\n",
    "            session.execute(query, row)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. inspect and validate the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(artist='Faithless', song='Music Matters (Mark Knight Dub)', song_length=495.30731201171875)\n"
     ]
    }
   ],
   "source": [
    "query_1 = \"\"\"\n",
    "SELECT artist\n",
    ",    song\n",
    ",    song_length\n",
    "\n",
    "FROM\n",
    "    session_info\n",
    "    \n",
    "WHERE\n",
    "    session_id=338\n",
    "and item_in_session=4\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    rows = session.execute(query_1)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(artist='Down To The Bone', song=\"Keep On Keepin' On\", first_name='Sylvie', last_name='Cruz', item_in_session=0)\n",
      "Row(artist='Three Drives', song='Greece 2000', first_name='Sylvie', last_name='Cruz', item_in_session=1)\n",
      "Row(artist='Sebastien Tellier', song='Kilometer', first_name='Sylvie', last_name='Cruz', item_in_session=2)\n",
      "Row(artist='Lonnie Gordon', song='Catch You Baby (Steve Pitron & Max Sanna Radio Edit)', first_name='Sylvie', last_name='Cruz', item_in_session=3)\n"
     ]
    }
   ],
   "source": [
    "query_2 = \"\"\"\n",
    "SELECT artist\n",
    ",    song\n",
    ",    first_name\n",
    ",    last_name\n",
    ",    item_in_session -- to check sorting\n",
    "\n",
    "FROM\n",
    "    user_info\n",
    "    \n",
    "WHERE\n",
    "    user_id=10\n",
    "and session_id=182\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    rows = session.execute(query_2)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(first_name='Sara', last_name='Johnson')\n",
      "Row(first_name='Jacqueline', last_name='Lynch')\n",
      "Row(first_name='Tegan', last_name='Levine')\n"
     ]
    }
   ],
   "source": [
    "query_3 = \"\"\"\n",
    "SELECT first_name\n",
    ",    last_name\n",
    "\n",
    "FROM\n",
    "    user_info_per_song\n",
    "    \n",
    "WHERE\n",
    "    song='All Hands Against His Own'\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    rows = session.execute(query_3)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. close the session and cluster connection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
